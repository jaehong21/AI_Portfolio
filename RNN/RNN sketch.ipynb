{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# In Python"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 10\n",
    "# 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
    "input_size = 4\n",
    "# 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다\n",
    "hidden_size = 8\n",
    "# 은닉 상태의 크기 (= 메모리 셀의 용량)\n",
    "\n",
    "inputs = np.random.random((timesteps, input_size))\n",
    "# 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size))\n",
    "# 초기 상태는 0벡터로 초기화\n",
    "# 은닉 상태의 크기는 hidden_size로 생성"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(hidden_state_t)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "Wx = np.random.random((hidden_size, input_size))\n",
    "# (8,4)크기의 2D 텐서로 입력에 대한 가중치\n",
    "Wh = np.random.random((hidden_size, hidden_size))\n",
    "# (8,8)크기의 2D 텐서로 은닉 상태에 대한 가중치\n",
    "b = np.random.random((hidden_size)) \n",
    "# (8,) 크기의 1D 텐서 생성. 이 값은 bias\n",
    "\n",
    "print(np.shape(Wx))\n",
    "print(np.shape(Wh))\n",
    "print(np.shape(b))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
    "    total_hidden_states.append(list(output_t))\n",
    "    \n",
    "    print(np.shape(total_hidden_states))\n",
    "    # 각 시점인 t별 메모리 셀의 출력 크기는 (timestep, output_dim)\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
    "print(total_hidden_states)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.94759769 0.92039135 0.84887687 0.8710714  0.94901628 0.97574841\n",
      "  0.83188082 0.94969605]\n",
      " [0.99998644 0.99979748 0.99998103 0.99999444 0.99999968 0.99999797\n",
      "  0.99996689 0.99997905]\n",
      " [0.99998574 0.99973527 0.999991   0.99999252 0.99999975 0.99999657\n",
      "  0.99997489 0.99995691]\n",
      " [0.99998511 0.99979778 0.99998102 0.99998389 0.99999973 0.99999602\n",
      "  0.99991296 0.99996612]\n",
      " [0.99999328 0.99978166 0.99999037 0.99999196 0.99999947 0.99999769\n",
      "  0.99992899 0.9999635 ]\n",
      " [0.99998922 0.9998514  0.99998814 0.99999339 0.99999984 0.99999809\n",
      "  0.99996703 0.99998222]\n",
      " [0.99999075 0.99982614 0.99998874 0.99999239 0.99999972 0.99999786\n",
      "  0.99995372 0.99997641]\n",
      " [0.99995989 0.99969826 0.99996848 0.9999503  0.99999937 0.99998785\n",
      "  0.99988226 0.99994163]\n",
      " [0.99999406 0.9998525  0.99999196 0.99999593 0.99999981 0.99999876\n",
      "  0.99996751 0.9999819 ]\n",
      " [0.99996453 0.99980451 0.99997241 0.99997487 0.9999998  0.99999362\n",
      "  0.99994408 0.99997231]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# In Pytorch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "input_size = 4\n",
    "hidden_size = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "inputs = torch.Tensor(1, 10, 4)\n",
    "# (batch_size, time_steps, input_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "# batch_First=True로 입력 텐서의 첫번째 차원이 배치 크기임을 알려준다"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "outputs, _status = cell(inputs)\n",
    "# RNN 셀은 두 개의 입력값을 리턴한다\n",
    "# 첫번째는 모든 시점의 hidden state, 둘째는 마지막 timestep의 은닉상태이다."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "print(outputs.shape)\n",
    "# 10번의 t동안 8차원의 hidden state가 출력되었다는 의미"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 10, 8])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print(_status.shape)\n",
    "# 마지막 시점의 hidden state는 (1, 1, 8)의 크기를 가짐"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "eaec98fea9be1e38372de84421a682e4a5688dcfcb4da6eba8e4c001369120b1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}