{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어-영어 번역 관련 '자연어 처리' 데이터셋은 'AI 허브'라는 사이트에서 다운로드 받았다. <br />\n",
    "AI 허브에서는 음성/자연어 분야부터 컴퓨터 비전, 헬스케어 등 많은 분야의 국내 데이터셋 등을 무료로 다운받을 수 있다. <br />\n",
    "(회원가입 절차를 필요로 한다) <br />\n",
    "그래서, seq2seq(Sequence-to-Sequence)를 이용한 한영 기계번역기 제작을 위해서 AI 허브에서 한영 구어체 데이터셋을 다운로드했다. <br />\n",
    "그리고, 진행하는 프로젝트 이외에도 공식 Pytorch 사이트에서 제공하는 seq2seq 튜토리얼에서 제공하는 영어-프랑스어 데이터셋 또한 <br />\n",
    "테스트를 위해서 추가적으로 다운로드 받았다. <br />\n",
    "\n",
    "<br /> <br />\n",
    "\n",
    "\n",
    "영어-프랑스어 번역 데이터셋 출처: https://download.pytorch.org/tutorial/data.zip\n",
    "한국어-영어 번역 데이터셋 출처: https://aihub.or.kr/aidata/87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_1 = pd.read_excel(\"2_구어체(1).xlsx\")\n",
    "excel_2 = pd.read_excel(\"2_구어체(2).xlsx\")\n",
    "excel_result = excel_1\n",
    "\n",
    "excel_result.to_csv('kor-eng.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else: \n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>one-hot encoding 및 데이터셋 정제 준비</h4> <br />\n",
    "이번 task에서는 one-hot encoding을 이용하려고 한다. <br />one-hot encoding은 문자를 단어 0으로 이루어진 엄청 큰 벡터에서 특정 단어가 가진 특정한 index에만 1을 넣는 one-hot vector로 나타내는 것을 말한다. <br />\n",
    "학습 이전에 위 과정을 쉽게 하기 위하여, 클래스 'Lang'을 제작하였다. 단어를 index에 대응시킨 word2index와 index에 단어를 대응시킨 index2word를 선언한다.  <br />\n",
    "seq2seq에 필수적으로 들어가는 토큰인 \"SOS\"와 \"EOS\"는 미리 추가해놓고, 이를 포함하여 n_words는 2로 초기화시켜 놓았다. <br />\n",
    "함수 addSentence는 ' '를 기준으로 문장을 쪼개서 단어를 받는 함수이다. <br />\n",
    "addWord는 addSentence로부터 받은 word를 word2index와 index2word에 추가시키고, n_words에 1을 더해준다. <br />\n",
    "문장으로부터 받은 단어가 이미 word2index에 있다면, n_words에만 1 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9가-힣.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>문장 전처리</h4> <br />\n",
    "장부호와 알파벳을 제외한 모든 단어를 제거해준다. <br /> \n",
    "다시 한번 언급하자면, 컴퓨터는 소문자와 대문자가 섞여있다면, 다른 단어로 인식한다. <br />\n",
    "또한, 문장부호가 붙어있는 것도 마찬가지이다. 그리하여, 납득할만한 결과를 얻기 위하여 이러한 데이터 전처리 과정이 필수적이다. <br />\n",
    "(한국어를 다룰 때에는 형태소 분석기 사용을 추천한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else: \n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>엑셀파일로부터 데이터 추출</h4> <br />\n",
    "본격적으로 data를 추출하는 단계이다. data.zip의 압축을 풀고, python 파일과 같은 디렉토리 내에 kor-eng.txt와 같이 저장해준다.<br /> .read().strip().split('\\n)을 통해서 메모장에 있는 파일을 줄 단위로 받아온다. (.strip()은 문자열의 양 끝에 있는 공백 혹은 \\n을 제거해주는 함수이다. <br />\n",
    "그 다음에 한 줄에서 한글 문장 한 개와 영어 문장 한 개 짝을 이루어서 받아오는 pair를 만들어준다. <br />\n",
    "메모장에서 한 줄에 한글과 영어가 tab으로 구분되어 있어 .split('\\t')으로 pair를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "'''\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "'''\n",
    "\n",
    "kor_prefixes= (\n",
    "    \"나는\", \"난\",\n",
    "    \"그는\", \"그녀는\",\n",
    "    \"우리는\", \"그들은\",\n",
    "    \"너는\", \"넌\",\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[0].startswith(kor_prefixes)\n",
    "    # and p[1].startswith(eng_prefixes)\n",
    "    # 만약 reverse=False면 p[0].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본래는 모든 데이터를 학습시키는 데에는 꽤 오랜 시간을 요구한다. 이러한 시간을 조금 줄이고자, 몇가지 필터링을 추가시키고자 한다. <br />\n",
    "1. 문장의 길이가 MAXLENGTH 이하인 문장만 학습한다. <br />\n",
    "2. eng_prefixes를 선언하여, 이의 형태로 시작하는 문장만 골라서 학습한다. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 199999 sentence pairs\n",
      "Trimmed to 28557 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "kor 35037\n",
      "eng 12080\n",
      "['우리는 생로랑과 구찌 가방도 필요합니다 .', 'We also need Saint Laurent and Gucci bags .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('kor', 'eng', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kor</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>우리는 언제나 서로를 사랑하려고 노력해요 .</td>\n",
       "      <td>We always try and show our love for each other .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>난존은 당신의 소중한 생각들을 위한 공간입니다 .</td>\n",
       "      <td>Nan Zone is a space for your precious thoughts .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>난존을 당신의 SNS에 공유해 주세요 .</td>\n",
       "      <td>Share Nanzon on your SNS account .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>넌 프랑스 어느 도시에서 왔어 ?</td>\n",
       "      <td>Which city in France are you from ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>너는 취미가 뭐야 ?</td>\n",
       "      <td>What is your hobby ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28552</th>\n",
       "      <td>나는 먼저 청소기로 바닥을 밀었어요 .</td>\n",
       "      <td>First of all I vacuumed the floor .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28553</th>\n",
       "      <td>나는 먼저 팀 과제를 하고 놀러 갔어요 .</td>\n",
       "      <td>I did the team assignment first and went out t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28554</th>\n",
       "      <td>나는 비 같은 멋진 연예인을 좋아해요 .</td>\n",
       "      <td>I like cool entertainer like Rain .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28555</th>\n",
       "      <td>나는 멋진 자연 경치를 보고 눈물을 흘렸어 .</td>\n",
       "      <td>I cried seeing the amazing scenery .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28556</th>\n",
       "      <td>나는 멋진 중학교 생활을 기대합니다 .</td>\n",
       "      <td>I look forward to a great middle school experi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28557 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               kor  \\\n",
       "0         우리는 언제나 서로를 사랑하려고 노력해요 .   \n",
       "1      난존은 당신의 소중한 생각들을 위한 공간입니다 .   \n",
       "2           난존을 당신의 SNS에 공유해 주세요 .   \n",
       "3               넌 프랑스 어느 도시에서 왔어 ?   \n",
       "4                      너는 취미가 뭐야 ?   \n",
       "...                            ...   \n",
       "28552        나는 먼저 청소기로 바닥을 밀었어요 .   \n",
       "28553      나는 먼저 팀 과제를 하고 놀러 갔어요 .   \n",
       "28554       나는 비 같은 멋진 연예인을 좋아해요 .   \n",
       "28555    나는 멋진 자연 경치를 보고 눈물을 흘렸어 .   \n",
       "28556        나는 멋진 중학교 생활을 기대합니다 .   \n",
       "\n",
       "                                                     eng  \n",
       "0       We always try and show our love for each other .  \n",
       "1       Nan Zone is a space for your precious thoughts .  \n",
       "2                     Share Nanzon on your SNS account .  \n",
       "3                    Which city in France are you from ?  \n",
       "4                                   What is your hobby ?  \n",
       "...                                                  ...  \n",
       "28552                First of all I vacuumed the floor .  \n",
       "28553  I did the team assignment first and went out t...  \n",
       "28554                I like cool entertainer like Rain .  \n",
       "28555               I cried seeing the amazing scenery .  \n",
       "28556  I look forward to a great middle school experi...  \n",
       "\n",
       "[28557 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df(pairs)\n",
    "dataset.columns = ['kor', 'eng']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제작한 pairs를 pandas의 DataFrame 형태로 전환한다. <br />\n",
    "'2_Data preparation_Dataset'라는 이름으로 xlsx와 csv 형식으로 파일을 저장해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_excel('2_Data preparation_Dataset.xlsx')\n",
    "dataset.to_csv('2_Data preparation_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
