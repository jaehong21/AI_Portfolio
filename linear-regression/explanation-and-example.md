---
description: Linear Regression 구현을 위해 선형 회귀에 대한 기본적인 배경지식과 원리를 익히는 것을 목표로 하는 페이지입니다.
---

# Basic Information

## 선형 회귀(Linear Regression) 

머신러닝은 이미 존재하는 데이터를 기반으로 모델로 생성하여, 입력값을 토대로 출력값을 예측하는 데에 목적을 두고 있습니다. 선형 회귀는 이러한 머신러닝 알고리즘의 일종입니다. 회귀 분석은 독립변수를 기반으로 목표 예측값을 모형화 합니다. 회귀 모형의 형태는 선 뿐만이 아니라, 독립 변수의 수 혹은 종속 변수와의 관계에 따라 달라질 수 있습니다.

![](<../.gitbook/assets/image (44).png>)

가장 직관적인 예시는 바로 1차 함수적 모델, 선입니다. 선형 회귀는 데이터를 가장 잘 설명할 수 있는 선을 찾는 통계기법이라고 할 수 있습니다. 물론 그 예측값이 정확하지 않을 수 있습니다. 위와 같은 1차 함수 H(x) = Wx + b 형태를 띠고, 우리는 머신러닝을 이용하여 데이터를 가장 잘 설명하고, 예측할 수 있는 W, b 값을 구하고자 하는 것입니다. 앞으로, 이러한 최적의 W, b를 찾아나가는 과정을 알아가보도록 합시다. 

### 오차 및 손실 (Loss)

위 그림에서 봤듯이, 아무리 완벽한 모델을 찾으려고 해도, 선을 벗어나는 값들이 있기 마련입니다. 우리는 선과 데이터 사이의 오차(거리)를 머신러닝에서 Loss라고 부를 수 있습니다. Loss의 정도를 표시할 때에는 선과 데이터의 차이를 제곱해줍니다. 손실을 더 돋보이게 하는 효과가 있기 때문입다. 이러한 방식을 **MSE (Mean Squared Error)**라고 합니. (제곱을 하지 않고 절댓값만 처리하는 MAE 방식 또한 존재합니다) 

![](<../.gitbook/assets/image (45).png>)

위에 제시된 함수를 **Cost Function**이라고 부르기도 합니다. 결국 데이터를 가장 잘 나타내는 선을 만든다는 것은 Cost Function을 최소화하는 parameter들을 찾는 것과 같습다. 

### 경사 하강법 (Gradient Descent)

머신러닝에서 사용하는 모형은 아주 많고 복잡하다고 합니. 그리하여, 선형 회귀 분석에 있어서 최적의 W와 b를 찾는 정해진 방법은 없습니다. 우리는 그 중 하나인 '경사 하강법'을 이용하고자 합니다. Gradient Descent는 Cost Function을 최소화하는 알고리즘들 중 하나이며, 머신러닝 이외에도 많은 최소화 문제에도 이용됩니다.

$$
cost = {1 \over n} \sum_{i=1}^n ( (Wx^{(i)}+b) - y^{(i)})^2
$$

일반적인 경우에는 W와 b를 변수로 하지만, 계산을 간단히 하기위해 일차함수 H(x) = Wx로 간략화 하겠습니다. 결국, 이 Cost Function은 W를 변수로 한 이차방정식의 형태라고 할 수 있습니다. 임의의 점에서 시작하여 기울기를 계산한 이후, W와 b를 조금씩 변화를 주면서, 기울기의 값이 가장 적은 지점을 찾아가도록 합니다. 위와 같은 과정을 반복하면서, Cost Function의 기울기를 계속하여 감소시키는 동시에, 가장 기울기가 낮은 지점을 갖는 W와 b의 값을 찾을 수 있게 합니다. 밑에 제시되어 있는 사진과 같이 기울기가 가장 낮은 지점이 Cost Function의 최소점과 거의 일치함을 볼 수 있습니다.

![](<../.gitbook/assets/image (46).png>)

$$
W:= W - \alpha {\partial \over\partial W} cost(W)
$$

경사 하강법의 공식입니다. W 값을 현재의 Cost Function 값의 미분 값의 α (Learning rate)를 곱한 값을 뺀 값으로 계속하여 교체합니다. 여기서 임의의 값 α는  Learning rate 혹은 학습률이라고 불립니다. Learning rate는 너무 작지도 않고, 크지도 않는 특정한 값을 설정해야 합니다. 만약, 너무 작은 값을 설정한다면, 기울기가 하강하는 속도가 너무 느립니다. 즉, 학습하는 속도가 매우 느려집니다. 반대로, Learning rate를 너무 큰 값으로 설정한다면, 우리가 목표로하는 최솟값을 바로 지나쳐버릴 수도 있습니다. 결국 여러 번의 시도를 통해, 적절한 학습률을 찾는 것 또한 중요합니다.  

### 수렴 (Convergence) 

그렇다면, 도대체 언제까지 작업을 반복해야할까?\
작업을 반복하면 할수록, 더 정교한 parameter 값을 얻을 수 있겠지만, 일정 횟수를 넘어가면 어차피 W와 b의 값은 정해진 값으로 수렴하며, 반복의 의미가 무색해질 것입니다.  결국, 오차의 범위가 너무 커지지 않는 선을 지키는 동시에,  학습하는 시간 모두 고려해야 하, 반복/학습 횟수(epoch)를 정해야 할 것입니다. \


#### +) 상관 계수 (correlation coefficient)

상관계수란, 말 그대로 두 변수의 상관 관계의 정도를 나타내기 위한 지표입니다. +1에 가까울수록 양의 상관관계를 갖고, -1에 가까울수록 음의 상관관계를 갖게 됩니다. 상관계수에도 여러 종류가 존재하지만, 가장 보편적인 것은 Karl Pearson이 제안한 **피어슨 상관계수**입니다.

### ~~Multivariable Linear Regression~~
